---
title: "Lecture 1: Outline of the planned work + linear regression"
author: "Dr Nemanja Vaci"
institute: "University of Sheffield"
date: "11/01/2021"
output:
  xaringan::moon_reader:
    css: [default, default-fonts, my-theme.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
      countIncrementalSlides: false
    seal: false

---

class: center
background-image: url("mainFinal.png")

???
https://app.wombo.art/
---

<style type="text/css">
body, td {
   font-size: 15px;
}
code.r{
  font-size: 15px;
}
pre {
  font-size: 20px
}
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 80% !important;
}
</style>

## Press record

```{r metathis, echo=FALSE, message=FALSE, warning=FALSE}
library(metathis)
meta() %>%
  meta_name("github-repo" = "nvaci/Lecture_1") %>% 
  meta_social(
    title = "Lecture 1: Outline of the planned work + linear regression",
    description = paste(
      "First lecture for the Advanced Statistics course at Psychological Research Methods in Psychology master course",
      "Sheffield 2021."
    ),
    url = "https://nvaci.github.io/Lecture_1/Index.html#1",
    image= "https://nvaci.github.io/Lecture_1/Index.html#1",
    og_type = "website",
    og_author = "Nemanja Vaci",
    twitter_card_type = "summary_large_image",
    twitter_site = "@nemanja_vaci"
  )
```

---
## Logistics

- What are we planning to talk about : https://nemanjavaci.netlify.app/advanced-stats-course/course-handbook/<br/> <br/>
  a) Generalized linear models <br/>
  b) Structural equation modelling: path models and confirmatory factor analysis <br/>
  c) Mixed-effects models: cross-sectional and longitudinal data <br/> <br/>
- Theory and practice (1 + 1 hour):<br/><br/>
a) Theoretical aspect – why and when would we want to use a certain statistical model <br/>
b) Mathematical aspect – the mathematical basis of the model and how can we transform the data and parameters <br/>
c) Practical aspect – using R to analyse the data and build the statistical models on real-world data
<br/>
<br/>
- R statistical environment
<br/>
<br/>
- Materials:<br/>
a) Presentations (press __p__ for additional content)<br/>
b) Commented R code
([link](https://nvaci.github.io/Lecture_1_code/Lecture_1_Code.html) for this lecture)<br/>
c) Slack channel to discuss statistical questions<br/>
d) Course glossary: link
???
General overview of the course<br/>
<br/>
Focus on theory and applications, but also on how to use these models in the R environment<br/>
<br/>
Each lecture is planned to be supported by HTML presentation with main slide content and lecturer's notes + commented code that produced outputs in the presentation. Often, I will include additional content that you can explore, but it is not going to be used for any subsequent knowledge assessments. 
---
## Knowledge assessment

####The application of statistical methods to the existing data
Set with a series of research questions for which you will have to propose a statistical model that tests hypothetical assumptions, motivate their models, build them in the R statistical environment, and interpret results (parameters, fit and critique of the model)

Homework: 10% of the final mark <br/>
Final essay type exam: 90% of the final mark

The exam will focus on the: <br/>
a) Theoretical aspect <br/>
b) Mathematical aspect <br/>
c) Practical aspect 

---
## Opportunities for feedback:

Discussions during the classes <br/>
Feedback on the homework <br/>
Slack channel and Taking stock <br/>

Course communication: n.vaci@sheffield.ac.uk <br/>
Office hours: Friday from 1 to 2 pm (https://tinyurl.com/y2ptqmwx)<br/>
Post your questions on the spreadsheet queries

---

## Today: linear regression

Intended learning outcomes: <br/>

Explain and motivate the linear regression model and how it relates to other statistical methods <br/> 

Build a model; analyse and interpret the coefficients (model with one predictor, categorical predictors, multiple predictors, and interactions) <br/>

Interpret other information that linear regression reports: determination coefficient, F-test, residuals <br/>

Evaluate fit and assumptions of the linear regression model <br/>

---
class: center, inverse
background-image: url("main.gif")
---
## Why do we use statistical models?
--
```{r, fig.align='center', echo=FALSE, out.width='40%'}
knitr::include_graphics('Distributions.jpeg')
```

Full image: [link](https://miro.medium.com/max/1400/1*NhLlwFMzN0yWSvhipqMgfw.jpeg)
---

## Gaussian (normal) distribution

$$y_ \sim \mathcal{N}(\mu,\sigma)$$

```{r, fig.align='center', echo=FALSE, out.width='90%'}
knitr::include_graphics('normalD.png')
```


???
Central limit theorem: [link](https://en.wikipedia.org/wiki/Central_limit_theorem)<br/><br/>

What can we do with one distribution: [Moments](https://en.wikipedia.org/wiki/Moment_(mathematics), [Normality](https://en.wikipedia.org/wiki/Normality_test#:~:text=In%20statistics%2C%20normality%20tests%20are,set%20to%20be%20normally%20distributed.)<br/>
What can we do with two? With three or more? <br/>
---

## Linear regression

Method that summarises how the average values of numerical outcome variable vary over values defined by linear functions of predictors ([visualisation](https://seeing-theory.brown.edu/regression-analysis/index.html#section1)) <br/> <br/>

--

$$ y = \alpha + \beta * x + \epsilon $$ 
--
```{r, fig.width=8, fig.height=5,fig.align='center', echo=FALSE}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
data("cars")
plot(cars$speed, cars$dist, xlab='Predictor', ylab='Outcome')
abline(lm(dist~speed, data=cars), lwd=2)
```
???
The mathematical equation that estimates a dependent variable Y from a set of predictor variables or regressors X.<br/>

Each regressor gets a weight/coefficient that is used to estimate Y. This coefficient is estimated according to some criteria, eg. minimises the sum of squared errors or maximises the logarithm of the likelihood function. Additional reading: <br/> [Estimation of the parameters](https://data.princeton.edu/wws509/notes/c2s2) 
<br/><br/>

$\epsilon$ - measure of accuracy of the model<br/>
$SS_{residual}=\sum_{i=1}^{n}(Y_{i}-\hat{Y}_{i})^2=\sum_{i=1}^{n}e_{i}^2$<br/>
<br/>
Other options: https://en.wikipedia.org/wiki/Deming_regression
---

## Example

Predict the height (cm) of babies from their age (months), weight (grams) and sex:

```{r,echo=F}
set.seed(456)
Babies=data.frame(Age=round(runif(100,1,30)), Weight=rnorm(100,4000,500))
Babies$Height=rnorm(100,40+0.2*Babies$Age+0.004*Babies$Weight, 5)
Babies$Sex=factor(rbinom(100,1,0.5))
levels(Babies$Sex)=c('Girls','Boys')
```

```{r,echo=T, fig.width=14, fig.height=5, fig.align='center'}
par(mfrow=c(1,3), bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
plot(Babies$Age, Babies$Height, xlab='Age (months)', ylab='Height (cm)')
plot(Babies$Weight, Babies$Height, xlab='Weight (grams)', ylab='Height (cm)')
boxplot(Babies$Height~Babies$Sex,xlab='Sex', ylab='Height (cm)')
```

---

## Interpretation: one predictor

$$ y = \alpha + \beta * Age + \epsilon $$ 

```{r}
lm1<-lm(Height~Age, data=Babies)
lm1$coefficients
```

Counterfactual interpretation (causal): Increase of 1 unit in predictor value - Age (being one month older) changes the outcome by the $\beta$ (model estimated value) <br/> <br/>

Predictive interpretation (descriptive): If we compare babies that differ in their age by 1 month, we expect to see that older babies are taller by $\beta$ on average <br/> <br/>

???
Model summarises the difference in average Height between the children as a function of their Age. <br/> <br/>
Intercept is the average (predicted) score for children at birht (0 - months)<br/> <br/>

Type of interpretation usually depends on the strength of the theoretical framework and methodological design. If you have well developed theoretical assumptions and experimental design, this perhaps allows you to use counterfactual interpretation.  

---

## Interpretation: multiple predictors

$$y = \alpha + \beta_1 * Age + \beta_2 * Weight + \epsilon$$ 
Interpretation becomes contingent on other variables in the model <br/> <br/>

```{r}
lm2<-lm(Height~Age+Weight, data=Babies)
lm2$coefficients
```

Age: Comparing babies that have same Weight, but differ in their Age by one month, the model predicts difference by a value of $\beta_1$ in their Height on average<br/> <br/>

Weight: Comparing babies that have same Age, but differ on their Weight by one gram, the model predicts difference by a value of $\beta_2$ in their Height on average

Regression coefficient is a [partial correlation](https://en.wikipedia.org/wiki/Partial_correlation) estimate 

???
```{r}
AgeRes=residuals(lm(Age~Weight, data=Babies))
HeightRes=residuals(lm(Height~Weight, data=Babies))
lm(HeightRes~AgeRes)$coefficients
```
---

## Interpretation: multiple predictors

$$y = \alpha + \beta_1 * Age + \beta_2 * Sex + \epsilon$$ 

Interpretation becomes contingent on other variables in the model <br/> <br/>

```{r}
lm2<-lm(Height~Age+Sex, data=Babies)
lm2$coefficients
```

Age: Comparing babies that have identical Sex, but differ in their Age by one month, the model predicts difference by a value of $\beta_1$ in their Height on average<br/> <br/>

Sex: Comparing babies that have same Age, but have different Sex, the model predicts difference by a value of $\beta_2$ in their Height on average <br/> <br/>

???
The model summarises difference in average Height between the children as a function of their Age and Sex. <br/> <br/>

Intercept is the average (prediction) score for girls (0 on Sex variable) and that have 0 months (birth).

---

## Categorical predictors

$$y = \alpha + \beta_1 * Age + \beta_2 * Sex_{Boys} + \epsilon$$ 

What is the model doing:
.center[
<img src="CatPred.png", width = "30%">
]


Each level is assigned a value: Girls - 0, Boys - 1 <br/> <br/>
The slope coefficient $\beta_2$ models the difference between the two levels

???
Additional reading on type of dummy coding in R: [link](https://stats.idre.ucla.edu/r/modules/coding-for-categorical-variables-in-regression-models/)
---

## Interpretation: interactions

$$y = \alpha + \beta_1 * Age + \beta_2 * Sex_{Boys} + \beta_3 * Age*Sex_{Boys} + \epsilon$$ 

We allow the slope of age to linearly change across the subgroups of Sex variable<br/> <br/>

```{r}
lm3<-lm(Height~Age*Sex, data=Babies)
lm3$coefficients
```

Age: In the case of girls, comparing difference in babies older by month, the model predicts average difference by $\beta_1$ coefficient<br/> <br/>

Sex: Expected difference between girls at birth and boys at birth is $\beta_2$ coefficient<br/> <br/>

Age*Sex: Difference in the Age slope for Girls and Boys
???
Intercept is the predicted Girls Hight at birth <br/> <br/>
Predicted Height for Boys, when Age is 0: Intercept + $Sex_{boys}$ <br/> <br/>
Slope for the Boys: Age + Age: $Sex_{Boys}$
---

## Interpretation: interactions

What about by-linear continous interactions?

$$y = \alpha + \beta_1 * Age + \beta_2 * Weight + \beta_3 * Age*Weight + \epsilon$$ 

```{r}
lm4<-lm(Height~Age*Weight, data=Babies)
lm4$coefficients
```

Age: Keeping weight at value of 0 and comparing babies that differ by 1 month in their age, the model predicts average difference of $\beta_1$ 

Weight: Keeping age at value of 0 (birth) and comparing babies that differ by 1 gram in their weight, the model predicts average difference of $\beta_2$

Age*Weight: The average difference between babies that differ by 1 month in their age, changes by $\beta_3$ with every 1 gram change in babies weight

???
By including an interaction we allow a model to be fit differently to subsets of data.<br/> <br/>
When to test the interactions: does the theory/clinical practice/previous studies postulate possibilities of interaction existing between your main effects? If yes, then proceed with caution. <br/> <br/>
+ a good sign is when the main effects are relatively strong and large and they explain a large amount of variation in the data
---

## What other information do we get from linear model?

```{r}
lm1<-lm(Height~Age, data=Babies)
summary(lm1)
```

???
Residuals - we talk about that later <br/> <br/>
Coefficients with standard errors, as well as t-test and significance values. Test statistic (t-test) is used to test the significance of the predictor against 0. The reason why is it approximated with t distribution: [Link](https://stats.stackexchange.com/a/344008)<br/> <br/>
Residual standard error - estimate of the fit of our model: $\sqrt(\frac{SS_{residual}}{df})$
---

## Determination coefficient

$R^2 = 1 - \frac{SS_{residual}}{SS_{total}}$

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(ggplot2)
Babies$lm1=predict(lm1, newdata = Babies)
Babies$diff=Babies$lm1-Babies$Height
```

```{r,fig.width=12, fig.height=5, fig.align='center', warning=FALSE}
ggplot()+geom_linerange(data=Babies,aes(x=Age, ymin=Height,ymax=lm1,colour=diff), size=1.2)+geom_line(data=Babies,aes(x=Age, y=lm1), size=1.2)+geom_point(data=Babies, aes(Age, y=Height), size=2)+ylab('Height')+xlab('Age')+ggtitle('SS_residual')+theme(axis.title=element_text(size=14), axis.text =element_text(size=12))
```

---

## Determination coefficient

$R^2 = 1 - \frac{SS_{residual}}{SS_{total}}$

```{r}
lm0<-lm(Height~1, data=Babies)
summary(lm0)
```

```{r, echo=FALSE}
Babies$lm0=predict(lm0, newdata = Babies)
Babies$diff2=Babies$lm0-Babies$Height
```

---

## Determination coefficient

$R^2 = 1 - \frac{SS_{residual}}{SS_{total}}$

```{r,fig.width=10, fig.width=12, fig.height=5, fig.align='center', warning=FALSE}

ggplot()+geom_linerange(data=Babies,aes(x=Age, ymin=Height,ymax=lm0,colour=diff2), size=1.2)+geom_line(data=Babies,aes(x=Age, y=lm0), size=1.2)+geom_point(data=Babies, aes(Age, y=Height), size=2)+ylab('Height')+xlab('Age')+ggtitle('SS_total')+theme(axis.title=element_text(size=14), axis.text =element_text(size=12))
```

---

## Improvement in our prediction?

```{r, echo=FALSE}
Babies$diff3=Babies$lm1-Babies$lm0
```

$F = \frac{SS_m/df_m}{SS_r/df_r}$


```{r,fig.width=12, fig.height=5, fig.align='center', warning=FALSE}
ggplot()+geom_linerange(data=Babies,aes(x=Age, ymin=lm1,ymax=lm0,colour=diff3), size=1.2)+geom_line(data=Babies,aes(x=Age, y=lm0), size=1.2)+geom_line(data=Babies, aes(Age, y=lm1), size=1.3, linetype='dotdash')+geom_point(data=Babies, aes(x=Age, y=Height), size=0.9)+ylab('Height')+xlab('Age')+ggtitle('Improvement')+theme(axis.title=element_text(size=14), axis.text =element_text(size=12))
```

---

## Why is this important? 

The general linear model is "basis" for all other models covered by this course <br/> <br/>

In particular, more complex statistical models are often just generalisations of the general linear model <br/> <br/>

The same would apply to "simpler" procedures, such as correlations, t-test, ANOVA, ANCOVA etc. <br/> <br/>

```{r}
cor(Babies$Height,Babies$Weight)
```

```{r}
Babies$HeightStand=scale(Babies$Height, scale=TRUE, center=TRUE)
Babies$WeightStand=scale(Babies$Weight, scale=TRUE, center=TRUE)
lmCorr<-lm(HeightStand~-1+WeightStand, data=Babies)
lmCorr$coefficients
```
???
General linear model is a special case of the broad family of models (Generalized Linear Models - more next week)
---

## Asumptions

a) Error distribution: $\mathcal{N}^{iid}(0,\sigma^2)$ <br/> <br/>
b) Linearity and additivity <br/> <br/>
c) Validity <br/> <br/>
d) Lack of perfect multicolinearity <br/> <br/>

???
0 being mean of residuals is a by-product of OLS when we include intercept. <br/> Assumptioms of IID ~ independently and identically distributed residuals, are often problematic in psychology, when we have clustered answers. <br/><br/>

Linearity - outcome can be modelled as a linear function of separate predictors<br/>
Additivity - postulated linear function should not vary across values of other variables, if it does - we need to add interactions<br/>
Validity - data that you are measuring should reflect the phenomenon you are interested in<br/>

---
class: inverse, middle, center
# Practical aspect

---

#Practical work
Example dataset: Income inequality and rates of hate crimes -
[Article](https://fivethirtyeight.com/features/higher-rates-of-hate-crimes-are-tied-to-income-inequality/) and 
[Data](https://github.com/fivethirtyeight/data/tree/master/hate-crimes)

Reading local files to R:
```{r}
inequality<-read.table('inequality.txt',sep=',', header=T)
knitr::kable(head(inequality[,c(1,2,8,12)]), format = 'html')
```

---

##Dataset: outcome measures

Probability density plots: probability of the random variable falling within a range of values <br/>

```{r,fig.width=10, fig.height=5, fig.align='center'}
par(mfrow=c(1,2), bty='n',mar = c(5, 4, 1, .1), cex=1.1, pch=16)
plot(density(inequality$hate_crimes_per_100k_splc, na.rm = T), main='Crimes per 100k')
plot(density(inequality$avg_hatecrimes_per_100k_fbi, na.rm = T), main='Average Crimes')
```
???
Shape of the distribution, the most likely range of values, the spread of values, modality etc. 
---

##Dataset: some of the predictors

```{r,fig.width=10, fig.height=5, fig.align='center'}
par(mfrow=c(1,2), bty='n',mar = c(5, 4, 1, .1), cex=1.1, pch=16)
plot(density(inequality$median_household_income, na.rm = T), main='Income')
plot(density(inequality$gini_index, na.rm = T), main='Gini')
```

---

##Bivariate plots

Scatter plots:

```{r,fig.width=10, fig.height=5, fig.align='center'}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
plot(inequality$median_household_income, inequality$avg_hatecrimes_per_100k_fbi, xlab='Median household income',ylab='Avg hatecrimes')
```

---

##Bivariate plots

Scatter plots:

```{r,fig.width=10, fig.height=5, fig.align='center'}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
plot(inequality$gini_index, inequality$avg_hatecrimes_per_100k_fbi,xlab='Gini index',ylab='Avg hatecrimes')
```

---

##Correlations in the data


```{r}
cor(inequality[,c(2,8,12)], use="complete.obs")
```

???
In this situation, I am primarily using correlation to summarise collected data. High correlations might prepare us for collinearity issues in regressions, unexpected changes in the coefficients etc.  
---

## Linear regression in R

```{r}
mod1<-lm(avg_hatecrimes_per_100k_fbi~median_household_income, data=inequality)
summary(mod1)
```

---

## Adding more predictors

Model comparison works beyond null - intercept only model and models with one or more predictors <br/> <br/>


```{r}
mod2<-lm(avg_hatecrimes_per_100k_fbi~median_household_income+gini_index, data=inequality)
anova(mod2)
```

---

## Adding more predictors

We could use stepwise approach and build our models by sequentially adding the predictors

This allows us to see relative improvement that we get by adding gini index to the model with median household income

```{r}
anova(mod1,mod2)
```

---

## Main effects for two predictors

```{r}
summary(mod2)
```

---

## Interactions?

Should median household income be adjusted across the levels of gini index (or vice versa)?

```{r}
mod3<-lm(avg_hatecrimes_per_100k_fbi~median_household_income*gini_index, data=inequality)
anova(mod1,mod2, mod3)
```

---

## Interactions!

```{r, echo=FALSE}
summary(mod3)
```

---

## Visualisations of the predictions

```{r,fig.width=14, fig.height=5, fig.align='center', warning=FALSE, message=FALSE}
require(interactions)
interact_plot(mod3, pred=median_household_income, modx=gini_index, plot.points = T)
```

---

## Visualisations of the predictions

```{r, echo=FALSE}
simulatedD<-data.frame(median_household_income=rep(seq(35500, 76165, by=100), 13), gini_index=rep(seq(0.41,0.53, by=.01), each=407))
simulatedD$pred<-predict(mod3, newdata=simulatedD)
```

```{r,fig.width=14, fig.height=5, fig.align='center'}
p<-ggplot(simulatedD, aes(median_household_income, pred, color=gini_index,frame=gini_index))+geom_point()
plotly::ggplotly(p)
```

---

## Model diagnostics

```{r}
summary(mod3$residuals)
```

Studentized residuals

$\epsilon_{Ti}=\frac{\epsilon_i}{\sigma_{(-i)}\sqrt{1-h_i}}$

???
Studentized residual: division of a residual by an estimate of its standard devitation weighted by an estimate of leverage (hat values) <br/> <br/>

Delete the observations one at the time and fit the model on the n-1 observations
---
class: center, inverse
background-image: url("overfit.jpg")
---

## Normality

```{r,fig.width=10, fig.height=4,fig.align='center'}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
car::qqPlot(mod3)
```

[Cross-validated on residuals](https://stats.stackexchange.com/a/101290) and 
[Cross-validated shiny](https://xiongge.shinyapps.io/QQplots/)

---

## Homoscedasticity


```{r,fig.width=10, fig.height=4,fig.align='center'}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
car:: residualPlot(mod3, type='rstudent')
```

---

## Outliers and leverage points

```{r,fig.width=10, fig.height=6,fig.align='center'}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
car:: influenceIndexPlot(mod3, vars=c("Cook","studentized"))
```

???
Cook's Distance of observation __i__ is defined as a sum of all the changes in the regression model when observation __i__ is removed from it. 
---

## Outliers and leverage points

```{r, fig.width=10, fig.height=6,fig.align='center'}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
car:: influenceIndexPlot(mod3, vars=c("Bonf","hat"))
```
---

## Autocorrelation between the residuals

```{r, fig.width=10, fig.height=6,fig.align='center'}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
stats::acf(resid(mod3))
```
---

## Observed versus predicted values

```{r, fig.width=10, fig.height=6,fig.align='center'}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
plot(predict(mod3),mod3$model$avg_hatecrimes_per_100k_fbi, xlab='Predicted values (model 3)', ylab='Observed values (avg hatecrimes)')
```

???
[Further reading](https://www.sagepub.com/sites/default/files/upm-binaries/38503_Chapter6.pdf)<br/> <br/>

---

## Model evaluation

Rerun the model excluding leverage values
.tiny[
```{r}
mod3.cor<-lm(avg_hatecrimes_per_100k_fbi~median_household_income*gini_index, data=inequality, subset=-9)
summary(mod3.cor)
```
]
---

## Model evaluation

Interaction plot
.tiny[
```{r}
mod2.cor<-lm(avg_hatecrimes_per_100k_fbi~median_household_income+gini_index, data=inequality, subset=-9)
summary(mod2.cor)
```
]
---

## Take away message (important bits)

Linear model - continuous and categorical predictors <br/> <br/>
Interpreting estimated coefficients <br/> <br/>
Interpreting results (significance, determination coefficient, F-test, residuals) <br/> <br/>
Understanding assumptioms of linear model <br/> <br/>
Using linear models in R: overall and stepwise approach <br/> <br/><br/> <br/>

Higher proficiency: <br/> <br/>
Model diagnostics based on pre and post-modelling procedures  covered  by the lecture and beyond

---

## Discussion points

1. Approach <br/> <br/>
We have two options: <br/> <br/>
a) We meet every Friday, where I present the lecture (recorded) - a major gain is that we can always stop the presentation and discuss the content <br/> b) I pre-record the lecture and we use this time slot to discuss it - major gain, more time to discuss (workshop type approach), while the major drawback is that you need to watch the lecture before we meet <br/> <br/>
2. Based on today's lecture (take into account that I was not going in depths of linear regression), would you prefer more:<br/><br/>
a) Theory - mathematical aspect + explanations <br/>
b) Examples - range of different problems <br/>
c) Practice - more code and focus on understanding the data <br/>
d) Stay same, never change <br/> <br/>


---

## Literature

First and second chapter of "Data Analysis Using Regression and Multilevel/Hierarchical Models" by Andrew Gelman and Jennifer Hill <br/> <br/>

First three chapter of "Regression Analysis and Linear Models: Concepts, Applications, and Implementation" by Richard B. Darlington and Andrew F. Hayes <br/> <br/>

van Rij, J., Vaci, N., Wurm, L. H., & Feldman, L. B. (2020). Alternative quantitative methods in psycholinguistics: Implications for theory and design. Word Knowledge and Word Usage, 83-126.

---

## Exercise

1. Find a data with an interesting measures and load it in R [FiveThirtyEight](https://data.fivethirtyeight.com/), [Kaggle](https://www.kaggle.com/) <br/><br/> 

2. Make a histogram and density plot <br/> <br/>

3. Calculation correlations between two or more variables <br/><br/>

4. If there is published work that used the dataset, try to repeat the analysis (only few variables and only if it is linear regression) <br/><br/>

5. Make a model where you have high or a perfect multicolinearity between two predictors (you can simulate data) <br/><br/>

6. Make a model where you have $R^2=1$

---

# Thank you for your attention